# tsc-2020-experiment-material
This repository is related to the paper entitled "Specification and Automated Analysis of Inter-Parameter Dependencies in Web APIs". It contains the data of the experiment performed in the context of automated testing of RESTful APIs.


We performed two experiments in order to answer the following research questions:
  - **RQ1**: *What is the effectiveness of IDLReasoner in generating valid requests for real-world APIs with inter-parameter dependencies?*
  - **RQ2**: *What is the fault-finding capability of IDLReasoner in real-world APIs with inter-parameter dependencies?*

Next, we detail the data collected when executing the experiments, what they consist of and where they are located.


## Data location

All resources are placed inside the `target` directory. The `<EVALUATION_NAME>` identifies the experiment in terms of the service tested and how the test cases were generated. For example, `youtubeSearch_allNominalIgnoreDependencies` means that the YouTube-Search service was tested with 100% nominal test cases generated with random testing, i.e., ignoring inter-parameter dependencies. See sections [RQ1](#rq1) and [RQ2](#rq2) for more information on the semantics of the identifiers used.

For each experiment performed, the following resources are provided:
  - **Allure reports**. Location: `target/allure-reports/<EVALUATION_NAME>/index.html`. Graphical interface to visualize all test cases run on the API, which of them failed, and why. **NOTE**: Allure HTML files cannot be rendered locally, you must use a server for such purpose. Advice: open the files from an IDE such as IntelliJ IDEA.
  - **Test cases**. Location: `target/test-data/<EVALUATION_NAME>/test-cases.csv`. These are a representation of the HTTP requests sent to the API, without the response.
  - **Test results**. Location: `target/test-data/<EVALUATION_NAME>/test-results.csv`. These are a representation of the HTTP responses sent back by the API.
  - **Number of nominal and faulty test cases generated**. Location: `target/test-data/<EVALUATION_NAME>/nominal-faulty.csv`.
  - **Time report**. Location: `target/test-data/<EVALUATION_NAME>/time.json`. Includes information on the time taken for the whole experiment, the generation and the execution of test cases.
  - **Test coverage**. Location: `target/coverage-data/<EVALUATION_NAME>/test-coverage.json`. Test coverage achieved by the test cases, according to the criteria defined in [Test Coverage Criteria for RESTful Web APIs](https://dl.acm.org/doi/10.1145/3340433.3342822).


## RQ1

For each service under test, we compared the effectiveness of random testing and IDLReasoner to generate valid requests. Interestingly, we found that some of the services tested had dependencies not described in the documentation. This was observed when obtaining 400 status codes ("bad request") with some input combinations that should be valid according to the documentation. In order to assess the effect of the missing dependencies, we defined them in the specification and included them in the evaluation as variants of the original case studies. Therefore, we performed the following three experiments for each service under test:

  - *allNominalIgnoreDependencies* (**random testing**). 100% of nominal test cases, i.e., those aiming to obtain a successful response (2XX status code), are generated. Inter-parameter dependencies are ignored.
  - *allNominalOriginalDependencies* (**IDLReasoner**). 100% of nominal test cases are generated. Inter-parameter dependencies are taken into account for the generation of the test cases. **IMPORTANT: The dependencies are specified as they are *described in the documentation*.**
  - *allNominalCorrectDependencies* (**IDLReasoner**). This experiment is equivalent to the previous one, but was necessary for those APIs whose dependencies are incorrectly specified in the documentation. **IMPORTANT: As the dependencies are wrongly specified, they are fixed in order to generate the maximum number of valid test cases as possible.** Inter-parameter dependencies are taken into account for the generation of the test cases.
  

## RQ2

For each service under test, we compared the effectiveness of random testing and IDLReasoner to find faults. We generated 1000 nominal test cases and 1000 faulty test cases, 2000 test cases in total. Faulty test cases in random testing were generated by violating the specification of individual parameters, e.g., omitting a mandatory parameter in the API request. With IDLReasoner, however, faulty test cases were divided into two groups: 500 test cases following the same approach as in random testing, and 500 test cases violating one or more inter-parameter dependencies. In total, we performed the following four experiments for each service under test:

  - *allNominalIgnoreDependencies* (**random testing**). Same experiment performed in RQ1.
  - *allFaultyParametersIgnoreDependencies* (**random testing**). 100% of faulty test cases, i.e., those aiming to obtain an error response (4XX status code) are generated. To create a faulty test case, two steps are performed: 1) generate a random request (ignoring dependencies); and 2) apply a random mutation to a single parameter so that the test case turns faulty, e.g., by removing a required parameter or passing a parameter the wrong type.
  - *allNominalOriginalDependencies* (**IDLReasoner**). Same experiment performed in RQ1.
  - *allFaultyHalfHalfOriginalDependencies* (**IDLReasoner**). 100% of faulty test cases are generated. These are divided into two groups: half of them are generated as in the previous evaluation; and the other half are generated by violating one or more inter-parameter dependencies. **IMPORTANT: The dependencies are specified as they are *described in the documentation*.**
